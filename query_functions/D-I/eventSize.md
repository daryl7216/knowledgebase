# eventSize()

The `eventSize()` function determines the number of bytes an event's values use internally for disk storage. It does not count the bytes used for storing field names.

***

## Parameters

| Parameter | Type | Required/Default | Description |
| :--- | :--- | :--- | :--- |
| **as** | string | optional <br> default: `_eventSize` | The name of the output field that will store the event size. |

***

## Function Operation

This function adds a field to each event containing its on-disk storage size in bytes. It's important to note that this does not reflect the RAM usage of an event during a query, and as a result, events generated by aggregations will have a size of zero.

***

## Example

### Track Event Size Statistics Over Time

This example shows how to calculate the size of each event and then report statistical information about those sizes (max, percentiles) in a time chart. This can be useful for monitoring trends in event size.

* **Query Example**
    ```
    eventSize(as=eventSize)
    | timeChart(function=[max(eventSize), percentile(field=eventSize, percentiles=[50,75,90,99])])
    ```

* **Step-by-Step**
    1.  `eventSize(as=eventSize)`: This first step calculates the size of each individual event and stores it in a new field named `eventSize`.
    2.  `timeChart(...)`: The events, now containing their size, are piped into `timeChart()`. This function calculates two sets of statistics for each time bucket:
        * `max(eventSize)`: Finds the largest event size within that bucket.
        * `percentile(...)`: Calculates the 50th, 75th, 90th, and 99th percentiles for the event sizes in that bucket.

* **Summary and Results**
    The query generates a graph showing the relative sizes of events over time. This allows you to visualize trends, such as the maximum event size and the distribution of event sizes (e.g., 99% of events are smaller than X bytes), which can help identify performance issues related to large events.